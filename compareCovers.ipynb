{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://arxiv.org/pdf/1110.2515.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFile1 = ''\n",
    "saveFile2 = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare covers using NMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseAnswer(answer):\n",
    "    parsedAnswer = []\n",
    "    for line in answer:\n",
    "        line = line.split('\\t')\n",
    "        if len(line) >= 2:\n",
    "            parsedAnswer.append(line)\n",
    "\n",
    "    for l in parsedAnswer:\n",
    "        l[0] = l[0].replace(':', '').replace(' ', '')\n",
    "    return parsedAnswer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareCovers(cover1, cover2, folderNMI, NMI_type='NMI<Max>'):\n",
    "    command = folderNMI +'onmi ' + cover1 + ' ' + cover2\n",
    "    p = subprocess.run(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "    answer = p.stdout.decode('ascii').split('\\n')\n",
    "    parsedAnswer = parseAnswer(answer)\n",
    "    \n",
    "    if NMI_type == 'NMI<Max>':\n",
    "        return parsedAnswer[0][1]\n",
    "    elif NMI_type == 'lfkNMI':\n",
    "        return parsedAnswer[1][1]\n",
    "    elif NMI_type == 'NMI<Sum>':\n",
    "        return parsedAnswer[2][1]\n",
    "    else:\n",
    "        print('Wrong NMI_type!\\n')\n",
    "        return parsedAnswer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderNMI = '' #nmi file\n",
    "c1 = '' #cover1 file\n",
    "c2 = '' #cover2 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderNMI +'onmi ' + c1 + ' ' + c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.0591832'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compareCovers(c1,c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openCover(file):\n",
    "    with open(file, 'rb') as fp:\n",
    "        cover = pickle.load(fp)\n",
    "    return cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openGraph(file):\n",
    "    with open(file, 'rb') as fp:\n",
    "        graph = pickle.load(fp)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcDegreeCentrality(g):\n",
    "    grau = {}\n",
    "    for v in g:\n",
    "        grau[v] = len(g[v])\n",
    "    #sorted_grau = sorted(grau.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return grau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcNodesCentrality(g1,g2):\n",
    "    grau = {}\n",
    "    for v in g1:\n",
    "        grau[v] = len(g1[v])\n",
    "    for v2 in g2:\n",
    "        if v2 in grau:\n",
    "            grau[v2] = (len(g1[v2]) + len(g2[v2])) / 2\n",
    "        else:\n",
    "            grau[v2] = len(g2[v2])\n",
    "    return grau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterCentrality(cluster, g, nodeCentrality):\n",
    "    total = 0\n",
    "    for n in cluster:\n",
    "        total += nodeCentrality[n]\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcClustersCentralities(cover, g, nodeCentrality):\n",
    "    centralities = {}\n",
    "    for c,i in zip(cover, range(len(cover))):\n",
    "        centralities[i] = clusterCentrality(c, g, nodeCentrality)\n",
    "        \n",
    "    return centralities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comunitySimilarity(c1,c2, n1,n2,nodeCentrality, clustersCentralities1, clustersCentralities2):\n",
    "    similarity = 0\n",
    "    for n in c1:\n",
    "        if n in c2:\n",
    "            #similarity += 1\n",
    "            similarity += nodeCentrality[n]\n",
    "            \n",
    "    #return similarity / max(len(c1), len(c2))\n",
    "    return [similarity / max(clustersCentralities1[n1], clustersCentralities2[n2]), similarity / min(clustersCentralities1[n1], clustersCentralities2[n2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestComunitySimilarity(comunity, cover1, nodeCentrality, clustersCentralities1, clustersCentralities2):\n",
    "    higherSimilarity = -1\n",
    "    nCluster = -1\n",
    "    for c, i in zip(cover1, range(len(cover1))):\n",
    "        similarity = comunitySimilarity(c, comunity, nodeCentrality, clustersCentralities1, clustersCentralities2)\n",
    "        if similarity > higherSimilarity:\n",
    "            higherSimilarity = similarity\n",
    "            nCluster = i\n",
    "            \n",
    "    return higherSimilarity, nCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coverSimilarities(cover1, cover2, nodeCentrality, clustersCentralities1, clustersCentralities2, sizeThreshold=10):\n",
    "    all_similarities = []\n",
    "    \n",
    "    for c1, n1 in zip(cover1, range(len(cover1))):\n",
    "        if len(c1) >= sizeThreshold:\n",
    "            local_similarities = []\n",
    "            for c2, n2 in zip(cover2, range(len(cover2))):\n",
    "                if len(c2) >= sizeThreshold:\n",
    "                    local_similarities.append(comunitySimilarity(c1,c2,n1,n2, nodeCentrality, clustersCentralities1, clustersCentralities2))\n",
    "                else:\n",
    "                    local_similarities.append([0,0])\n",
    "            all_similarities.append(local_similarities)\n",
    "        else:\n",
    "            local_similarities = []\n",
    "            for c2 in cover2:\n",
    "                local_similarities.append([0,0])\n",
    "            all_similarities.append(local_similarities)\n",
    "        \n",
    "    return all_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareCovers(all_similarities, threshold):\n",
    "    \n",
    "    similar_clusters = []\n",
    "    for c1 in range(len(all_similarities)):\n",
    "        for c2 in range(len(all_similarities[c1])):\n",
    "            if all_similarities[c1][c2][0] >= threshold:\n",
    "                #if [c2,c1,all_similarities[c1][c2]] not in similar_clusters:\n",
    "                similar_clusters.append([c1,c2,all_similarities[c1][c2][0]])\n",
    "    \n",
    "    return similar_clusters            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cover1 = openCover('') #cover1 pickle file\n",
    "cover2 = openCover('') #cover2 pickle file\n",
    "g1 = openGraph('') #networkx graph1 pickle file\n",
    "g2 = openGraph('') #networkx graph1 pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodeCentrality = calcNodesCentrality(g1,g2)\n",
    "c1Centralities = calcClustersCentralities(cover1, g1, nodeCentrality)\n",
    "c2Centralities = calcClustersCentralities(cover2, g2, nodeCentrality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_similarities = coverSimilarities(cover1, cover2, nodeCentrality, c1Centralities, c2Centralities, sizeThreshold=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_threshold = 0.4\n",
    "similar_clusters = compareCovers(all_similarities, similarity_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(similar_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similar CLusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0.5857117427164303],\n",
       " [4, 1, 0.4581497797356828],\n",
       " [10, 10, 0.41357234314980795],\n",
       " [11, 3, 0.4150513112884835],\n",
       " [11, 6, 0.5568585643212509],\n",
       " [11, 8, 0.4012760241773002],\n",
       " [12, 4, 0.6664350243224462],\n",
       " [14, 9, 0.41645244215938304],\n",
       " [24, 32, 0.8148148148148148],\n",
       " [26, 33, 0.45012165450121655],\n",
       " [28, 16, 0.42718446601941745],\n",
       " [28, 37, 0.49299719887955185],\n",
       " [33, 38, 0.5144927536231884]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster y from cover 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cover1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-4bebae4b562b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m26\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcover1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mlista\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mg1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'peso'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cover1' is not defined"
     ]
    }
   ],
   "source": [
    "lista = []\n",
    "y = 26\n",
    "\n",
    "for n in cover1[y]:\n",
    "    lista.append([g1.nodes()[n]['peso'], n])\n",
    "    \n",
    "sorted(lista, key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster y from cover 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Features', 6380, 'bn:00017761n'],\n",
       " ['trained', 5235, 'bn:00086736v'],\n",
       " ['learned', 4888, 'bn:00082281v'],\n",
       " ['propose', 4325, 'bn:00082417v'],\n",
       " ['number', 3792, 'bn:00058285n'],\n",
       " ['compare', 2328, 'bn:00085450v'],\n",
       " ['select', 2327, 'bn:00084931v'],\n",
       " ['observe', 2295, 'bn:00086708v'],\n",
       " ['performs', 2279, 'bn:00087107v'],\n",
       " ['obtaining', 2225, 'bn:00091124v'],\n",
       " ['following', 2181, 'bn:00088421v'],\n",
       " ['computing', 2150, 'bn:00084373v'],\n",
       " ['provide', 2090, 'bn:00088643v'],\n",
       " ['consider', 2088, 'bn:00085647v'],\n",
       " ['need', 1920, 'bn:00082822v'],\n",
       " ['achieve', 1763, 'bn:00082226v'],\n",
       " ['find', 1717, 'bn:00084231v'],\n",
       " ['modes', 1506, 'bn:00033729n'],\n",
       " ['improved', 1352, 'bn:00082573v'],\n",
       " ['studied', 1144, 'bn:00082596v'],\n",
       " ['generated', 1089, 'bn:00084080v'],\n",
       " ['similar', 1087, 'bn:00110647a'],\n",
       " ['builds', 1038, 'bn:00084198v'],\n",
       " ['possible', 919, 'bn:00108829a'],\n",
       " ['determine', 871, 'bn:00082811v'],\n",
       " ['simple', 744, 'bn:00110649a'],\n",
       " ['activities', 636, 'bn:00001172n'],\n",
       " ['effective', 498, 'bn:00102038a'],\n",
       " ['efficient', 413, 'bn:00102049a'],\n",
       " ['eigenvalues', 357, 'bn:00017765n'],\n",
       " ['unsupervised', 264, 'bn:00113295a'],\n",
       " ['supervised', 258, 'bn:00111485a'],\n",
       " ['efficient', 161, 'bn:00102039a'],\n",
       " ['corresponding', 154, 'bn:00085837v'],\n",
       " ['feature learning', 134, 'bn:16846119n'],\n",
       " ['kinematic', 104, 'bn:13689687a'],\n",
       " ['such a', 60, 'bn:13604191a'],\n",
       " ['paved', 12, 'bn:00091490v']]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista = []\n",
    "y = 8\n",
    "\n",
    "for n in cover2[y]:\n",
    "    lista.append([dictionaryCodeMerged2[n], g2.nodes()[n]['peso'], n])\n",
    "    \n",
    "sorted(lista, key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodes that belong to Clusters i of Cover 1 and j from Cover 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['high', 813, 863],\n",
       " ['affinity', 79, 84],\n",
       " ['specificity', 76, 133],\n",
       " ['throughput', 31, 49],\n",
       " ['Salinity', 23, 45],\n",
       " ['sensitivity', 22, 30]]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 12\n",
    "j = 19\n",
    "lista_iguais = []\n",
    "for n in cover1[i]:\n",
    "    if n in cover2[j]:\n",
    "        lista_iguais.append([dictionaryCodeMerged2[n], g1.nodes()[n]['peso'], g2.nodes()[n]['peso']])\n",
    "        \n",
    "sorted(lista_iguais, key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View in Cytoscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mauro/.local/lib/python3.7/site-packages/python_igraph-0.7.1.post6-py3.7-linux-x86_64.egg/igraph/_igraph.cpython-37m-x86_64-linux-gnu.so: undefined symbol: igraph_layout_davidson_harel\n",
      "py2cytoscape: Error importing igraph. You won't be able to import from igraph.\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from py2cytoscape import cyrest\n",
    "from py2cytoscape import util as cy \n",
    "from py2cytoscape.data.cyrest_client import CyRestClient\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = cover1[14]\n",
    "c2 = cover1[19]\n",
    "c3 = cover2[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparison of two clusters from different covers\n",
    "new_graph = nx.Graph()\n",
    "for n in c1:\n",
    "    if n in c2:\n",
    "        new_graph.add_node(n, peso=g1.nodes()[n]['peso']+g2.nodes()[n]['peso'], clusters=3, dicionario=dictionaryCodeMerged1[n])\n",
    "    else:\n",
    "        new_graph.add_node(n, peso=g1.nodes()[n]['peso'], clusters=1, dicionario=dictionaryCodeMerged1[n])\n",
    "for n in c2:\n",
    "    if n not in c1:\n",
    "        new_graph.add_node(n, peso=g2.nodes()[n]['peso'], clusters=2, dicionario=dictionaryCodeMerged2[n])\n",
    "\n",
    "for e in g1.edges():\n",
    "    if new_graph.has_node(e[0]) and new_graph.has_node(e[1]):\n",
    "        if e in g2.edges():\n",
    "            new_graph.add_edge(e[0], e[1], weight= g1[e[0]][e[1]]['weight'] + g2[e[0]][e[1]]['weight'])\n",
    "        else:\n",
    "            new_graph.add_edge(e[0], e[1], weight= g1[e[0]][e[1]]['weight'])\n",
    "        \n",
    "for e in g2.edges():\n",
    "    if new_graph.has_node(e[0]) and new_graph.has_node(e[1]):\n",
    "        if e not in g1.edges(): \n",
    "            new_graph.add_edge(e[0], e[1], weight= g2[e[0]][e[1]]['weight'])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evolution of two clusters into one cluster\n",
    "new_graph = nx.Graph()\n",
    "count = 0 \n",
    "dicionario1={}\n",
    "dicionario2={}\n",
    "dicionario3={}\n",
    "for n in c1:\n",
    "    new_graph.add_node(count, code=n, peso=g1.nodes()[n]['peso'], clusters=1, dicionario=dictionaryCodeMerged1[n])\n",
    "    dicionario1[n] = count\n",
    "    count += 1\n",
    "for n in c2:\n",
    "    new_graph.add_node(count, code=n, peso=g1.nodes()[n]['peso'], clusters=2, dicionario=dictionaryCodeMerged1[n])\n",
    "    dicionario2[n] = count\n",
    "    count += 1\n",
    "for n in c3:\n",
    "    new_graph.add_node(count, code=n, peso=g2.nodes()[n]['peso'], clusters=3, dicionario=dictionaryCodeMerged2[n])\n",
    "    dicionario3[n] = count\n",
    "    count += 1\n",
    "    \n",
    "for n in c1:\n",
    "    if n in c2:\n",
    "        new_graph.add_edge(dicionario1[n], dicionario2[n])\n",
    "    if n in c3:\n",
    "        new_graph.add_edge(dicionario1[n], dicionario3[n])\n",
    "for n in c2:\n",
    "    if n in c3:\n",
    "        new_graph.add_edge(dicionario2[n], dicionario3[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nx.write_gml(new_graph, saveFile1 + 'cls_12_19.gml')\n",
    "nx.write_gml(new_graph, saveFile1 +'cls_14,19_5.gml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "cytoscape=cyrest.cyclient()\n",
    "cyjs = CyRestClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = cyjs.network.create_from_networkx(new_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create groups\n",
    "for i in range(1,4):\n",
    "    group = ''\n",
    "    for n in new_graph.nodes():\n",
    "        if new_graph.nodes()[n]['clusters'] == i:\n",
    "            group += 'name:' + str(n) + ','\n",
    "    group = group[:-1]\n",
    "    cytoscape.group.create(nodeList=group, groupName='group'+str(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'null'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionaryCodeMerged2['bn:00107646a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n",
      "1\n",
      "3\n",
      "3\n",
      "1\n",
      "3\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for n in new_graph.nodes():\n",
    "    print( new_graph.nodes()[n]['clusters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
